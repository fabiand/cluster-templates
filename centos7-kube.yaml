apiVersion: v1
kind: Template
metadata:
  name: kube-node
  annotations:
    openshift.io/display-name: "Red Hat Enterprise Linux 7.5 VM"
    description: >-
      This template can be used to create a VM suitable for
      Red Hat Enterprise Linux 7.5.
      The template assumes that a PVC is available which is providing the
      necessary RHEL disk image.
    tags: "kubevirt,virtualmachine,linux,rhel"
    iconClass: "icon-rhel"
    openshift.io/provider-display-name: "KubeVirt"
    openshift.io/documentation-url: "https://github.com/fabiand/common-templates"
    openshift.io/support-url: "https://github.com/fabiand/common-templates/issues"
    template.openshift.io/bindable: "false"
objects:
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    name: ${CLUSTERNAME}
  spec:
    selector:
      matchLabels:
        app: cluster # has to match .spec.template.metadata.labels
    replicas: 2 # by default is 1. 1 will be master
    serviceName: node
    template:
      metadata:
        labels:
          app: cluster # has to match .spec.selector.matchLabels
      spec:
        containers:
        - name: vmctl
          image: quay.io/fabiand/vmctl:v0.1.0
          args:
          - "${VMNAME}"
          volumeMounts:
          - name: podinfo
            mountPath: /etc/podinfo
        serviceAccountName: cluster-creator
        volumes:
        - name: podinfo  # For affinity
          downwardAPI:
            items:
            - path: "name"
              fieldRef:
                fieldPath: metadata.name
- apiVersion: kubevirt.io/v1alpha2
  kind: VirtualMachine
  metadata:
    name: ${VMNAME}
  spec:
    running: false
    template:
      spec:
        domain:
          cpu:
            cores: 4
          devices:
            disks:
            - disk:
                bus: virtio
              name: rootdisk
              volumeName: rootvolume
            - disk:
                bus: virtio
              name: cloud
              volumeName: cloudinitvolume
            - disk:
                bus: virtio
              name: sa
              serial: underk8ssa
              volumeName: serviceaccount
            interfaces:
            - bridge: {}
              name: default
              model: e1000
            rng: {}
          resources:
            requests:
              memory: 1.5G
            limits:
              memory: 2G
        terminationGracePeriodSeconds: 0
        networks:
        - name: default
          pod: {}
        volumes:
        - name: rootvolume
          ephemeral:
            persistentVolumeClaim:
              claimName: ${PVCNAME}
        - name: cloudinitvolume
          cloudInitNoCloud:
            secretRef:
              name: cloud-config-for-${CLUSTERNAME}
        - serviceAccount:
            serviceAccountName: cluster-creator
          name: serviceaccount
- apiVersion: v1
  kind: Secret
  metadata:
    name: cloud-config-for-${CLUSTERNAME}
  stringData:
    userdata: |
      #cloud-config
      
      password: centos
      chpasswd: {expire: False}
      ssh_pwauth: True
      ssh_authorized_keys:
        - $SSH_PUBKEY
      
      package_upgrade: false
      
      runcmd:
      - |
        KUBERNETES_VER=v1.11.4
        KUBEVIRT_VER=v0.9.3
        
        PATH=$PATH:/usr/local/bin
        
        set -ex
      
        id

        # from https://kubernetes.io/docs/setup/independent/install-kubeadm/
        yum install -y docker
        systemctl enable docker && systemctl start docker
        
        cat <<EOF > /etc/yum.repos.d/kubernetes.repo
        [kubernetes]
        name=Kubernetes
        baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
        enabled=1
        gpgcheck=1
        repo_gpgcheck=1
        gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
        exclude=kube*
        EOF
        
        setenforce 0
        yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
        systemctl enable kubelet && systemctl start kubelet
        
        cat <<EOF >  /etc/sysctl.d/k8s.conf
        net.bridge.bridge-nf-call-ip6tables = 1
        net.bridge.bridge-nf-call-iptables = 1
        EOF
        sysctl --system
        
        # Mount under cluster SA
        mkdir -p /var/run/secrets/kubernetes.io/serviceaccount/ || :
        mount /dev/disk/by-id/virtio-underk8ssa /var/run/secrets/kubernetes.io/serviceaccount/ || :
        sudo kubectl --kubeconfig=~/kubeconfig config set-cluster under --server=https://kubernetes.default.svc.cluster.local --certificate-authority=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        sudo kubectl --kubeconfig=~/kubeconfig config set-credentials under --token=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
        sudo kubectl --kubeconfig=~/kubeconfig config set-context under --cluster=under --namespace=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace) --user=under

        if [[ "$HOSTNAME" =~ .*-0 ]];
        then
          sudo kubectl --kubeconfig=~/kubeconfig --context=under delete secret kubevirt-kube-cluster-config-for-cluster-${CLUSTERNAME} || :

          kubeadm config images pull --feature-gates CoreDNS=false
          # avoid cycling dependency
          for image in $(curl https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml | egrep -o "image: .*" | sort -u | sed "s/image: //"); do docker pull $image ; done
          # coredns is getting killed due to oom
          kubeadm init --pod-network-cidr=10.244.0.0/16 --service-cidr=10.97.0.0/12 --feature-gates CoreDNS=false
        
          mkdir -p ~/.kube
          sudo cat /etc/kubernetes/admin.conf > ~/.kube/config
        
          # Remove taints in order to allow setup on single master - and then scale up
          sudo kubectl taint nodes --all node-role.kubernetes.io/master-

          # http://github.com/kubernetes/kubeadm/issues/1179
          #sudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml
          sudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
          #sudo kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(sudo kubectl version | base64 | tr -d '\n')"
       
          sudo kubectl create configmap -n kube-system kubevirt-config --from-literal debug.useEmulation=true || true
          sudo -E kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VER/kubevirt.yaml
  
          #sudo kubectl --kubeconfig=~/kubeconfig --context=under config view
          #sudo kubectl --kubeconfig=~/kubeconfig --context=under describe nodes

          KUBEADMTOKEN=$(kubeadm token create --ttl 1h --print-join-command)
          sudo kubectl --kubeconfig=~/kubeconfig --context=under apply -f - <<EOM
          apiVersion: v1
          kind: Secret
          metadata:
            name: kubeadm-join-${CLUSTERNAME}
          stringData:
            master: $HOSTNAME
            joinCommand: $KUBEADMTOKEN
          EOM
       
        else
        
          kGetConfig() { sudo kubectl --kubeconfig=~/kubeconfig --context=under get secret kubeadm-join-${CLUSTERNAME} $@ ; }
          getJoinCmd() { kGetConfig -o=jsonpath='{.data.joinCommand}' | base64 -d ; }
          JOIN_CMD=""
          while [[ -z $JOIN_CMD ]];
          do
            echo -n .
            JOIN_CMD=$(getJoinCmd)
            sleep 6
          done
          eval "sudo $JOIN_CMD"

        fi

parameters:
- name: VMNAME
  description: Name of the new VM _template_
  value: kubenode
  required: true
- name: PVCNAME
  description: Name of the PVC with the disk image
  required: true
- name: CLUSTERNAME
  description: Name of the cluster to create or join
  value: kubernetes
- name: SSH_PUBKEY
  description: Public key to grant access to
